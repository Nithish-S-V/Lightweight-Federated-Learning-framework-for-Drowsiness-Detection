{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24719868-5dff-446d-a7a7-ea9fe25ac0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import zlib\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from tensorflow.keras import layers, initializers, backend as K\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import base64\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc12ad63-eac5-45be-bf91-d1b2ce3392d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.keras.saving.register_keras_serializable(package=\"Custom\", name=\"binary_crossentropy_loss\")\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c40c327-f88a-438c-b549-7bdabe39320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Capsule Network Components\n",
    "@register_keras_serializable(package=\"Custom\")\n",
    "class Length(layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(Length, self).get_config()\n",
    "\n",
    "@tf.keras.saving.register_keras_serializable(package=\"Custom\")\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "        \n",
    "        self.W = self.add_weight(\n",
    "            shape=[1, self.input_num_capsule, self.num_capsule, self.dim_capsule, self.input_dim_capsule],\n",
    "            initializer=initializers.glorot_uniform(),\n",
    "            name='W'\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "        W_tiled = K.tile(self.W, [K.shape(inputs)[0], 1, 1, 1, 1])\n",
    "        inputs_hat = tf.squeeze(tf.matmul(W_tiled, inputs_expand, transpose_b=True), axis=-1)\n",
    "        b = tf.zeros(shape=[K.shape(inputs)[0], self.input_num_capsule, self.num_capsule])\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)\n",
    "            c_expand = K.expand_dims(c, -1)\n",
    "            outputs = self.squash(tf.reduce_sum(inputs_hat * c_expand, axis=1))\n",
    "            if i < self.routings - 1:\n",
    "                b += tf.reduce_sum(inputs_hat * K.expand_dims(c, -1), axis=-1)\n",
    "        \n",
    "        return outputs\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_capsule\": self.num_capsule,\n",
    "            \"dim_capsule\": self.dim_capsule,\n",
    "            \"routings\": self.routings\n",
    "        })\n",
    "        return config\n",
    "    def squash(self, vectors, axis=-1):\n",
    "        s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "        return scale * vectors\n",
    "\n",
    "@tf.keras.saving.register_keras_serializable(package=\"Custom\", name=\"margin_loss\")\n",
    "def margin_loss(y_true, y_pred):\n",
    "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=2)\n",
    "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
    "\n",
    "class MobileNetCapsNet:\n",
    "    def __init__(self, input_shape=(224, 224, 3)):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=self.input_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = layers.Conv2D(256, 3, activation='relu')(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Reshape((-1, 256))(x)\n",
    "        \n",
    "        x = CapsuleLayer(num_capsule=8, dim_capsule=16, routings=3)(x)\n",
    "        x = CapsuleLayer(num_capsule=2, dim_capsule=32, routings=3)(x)\n",
    "        outputs = Length()(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the model with appropriate loss and optimizer\"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=self.margin_loss,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def margin_loss(y_true, y_pred):\n",
    "        \"\"\"Margin loss for capsule network\"\"\"\n",
    "        # Convert y_true to one-hot if it isn't already\n",
    "        if len(K.int_shape(y_true)) == 1:\n",
    "            y_true = tf.one_hot(tf.cast(y_true, 'int32'), 2)\n",
    "            \n",
    "        L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
    "            0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
    "        return tf.reduce_mean(tf.reduce_sum(L, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01170038-0f73-46db-8f73-d104fdaaee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Load pre-trained global model\n",
    "def load_model_from_file():\n",
    "    return load_model(r\"D:\\Major Project\\Rasp\\old\\drowsiness_model_teacher_our_final_10_epoch.keras\", \n",
    "                      custom_objects={\"CapsuleLayer\": CapsuleLayer,\"Length\":Length,\"margin_loss\":margin_loss})#,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99ec35c-d875-4fa7-9d39-bba876d1c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "BATCH_SIZE = 16\n",
    "TEST_DIR = r\"D:\\Major Project\\Rasp\\old\\test\"\n",
    "# Evaluate the global model\n",
    "def evaluate_model(model, dataset_dir):\n",
    "# Define test image generator\n",
    "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "    test_gen = test_datagen.flow_from_directory(\n",
    "        dataset_dir,\n",
    "        target_size=INPUT_SHAPE[:2],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(test_gen), axis=1)\n",
    "    y_true = test_gen.classes\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"Accuracy: {np.mean(y_true == y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Not Drowsy', 'Drowsy']))\n",
    "# Server socket for federated learning\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "375e77ee-3145-4c00-8234-f7c638e8c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Major Project\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Found 2374 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Major Project\\venv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 282ms/step\n",
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.8997\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Not Drowsy       0.94      0.86      0.90      1223\n",
      "      Drowsy       0.87      0.94      0.90      1151\n",
      "\n",
      "    accuracy                           0.90      2374\n",
      "   macro avg       0.90      0.90      0.90      2374\n",
      "weighted avg       0.90      0.90      0.90      2374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_model = load_model_from_file()\n",
    "evaluate_model(global_model,TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b34b7c-5c7f-4df8-b1e0-1a8a91dcc235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7596 images belonging to 2 classes.\n",
      "Found 1898 images belonging to 2 classes.\n",
      "Found 2374 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir1 = r\"D:\\Major Project\\Rasp\\old\\initial_train\"\n",
    "test_dir1 =  r\"D:\\Major Project\\Rasp\\old\\test\"\n",
    "\n",
    "# Define ImageDataGenerators for training, validation, and testing\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Split for validation\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create training, validation, and test generators\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_dir1,\n",
    "    target_size=INPUT_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    train_dir1,\n",
    "    target_size=INPUT_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    test_dir1,\n",
    "    target_size=INPUT_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False  # Ensures label order consistency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854f5d82-2eaf-4968-997c-a3f22830c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "class MobileNetStudent:\n",
    "    def __init__(self, input_shape=(224, 224, 3), num_classes=2, learning_rate=3e-4):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "        self._compile_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=self.input_shape,\n",
    "            include_top=False,\n",
    "            weights=\"imagenet\",  # Use pre-trained weights\n",
    "            alpha=0.35  # Reduced alpha to make it smaller\n",
    "        )\n",
    "        base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dropout(0.6)(x)  # Increased dropout\n",
    "        x = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)  # Reduced units\n",
    "        outputs = layers.Dense(self.num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        return Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    def _compile_model(self):\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"CategoricalAccuracy\"]\n",
    "        )\n",
    "\n",
    "    def get_callbacks(self):\n",
    "        lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=2, verbose=1\n",
    "        )\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            \"best_model.h5\", save_best_only=True, monitor=\"val_loss\", mode=\"min\"\n",
    "        )\n",
    "        return [lr_scheduler, early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b2c3e1-0f99-4527-9987-bbb24b71e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37663af5-0fbf-4e0c-8844-b31d51b25692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Major Project\\venv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 330ms/step - categorical_accuracy: 0.6593 - loss: 0.7863 - val_categorical_accuracy: 0.6185 - val_loss: 0.6925\n",
      "Epoch 2/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 345ms/step - categorical_accuracy: 0.7887 - loss: 0.5014 - val_categorical_accuracy: 0.6349 - val_loss: 0.8244\n",
      "Epoch 3/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 319ms/step - categorical_accuracy: 0.7678 - loss: 0.4896 - val_categorical_accuracy: 0.6175 - val_loss: 0.8169\n",
      "Epoch 4/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 292ms/step - categorical_accuracy: 0.8309 - loss: 0.4202 - val_categorical_accuracy: 0.6628 - val_loss: 0.7458\n",
      "Epoch 5/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 286ms/step - categorical_accuracy: 0.8098 - loss: 0.4452 - val_categorical_accuracy: 0.6433 - val_loss: 0.7142\n",
      "Found 2374 images belonging to 2 classes.\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 232ms/step\n",
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.8239\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Not Drowsy       0.88      0.77      0.82      1223\n",
      "      Drowsy       0.78      0.89      0.83      1151\n",
      "\n",
      "    accuracy                           0.82      2374\n",
      "   macro avg       0.83      0.83      0.82      2374\n",
      "weighted avg       0.83      0.82      0.82      2374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student = MobileNetStudent()\n",
    "model_student = student.model\n",
    "model_student.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "    loss=CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[CategoricalAccuracy()]) # Load once at the start\n",
    "history2 = model_student.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=5,\n",
    "        verbose=1)\n",
    "model_student.save(r\"D:\\Major Project\\Rasp\\model\\drowsiness_student_pre_model_5_epochs_wo_callbacks.keras\")\n",
    "evaluate_model(model_student,test_dir1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9b411f-b8b8-4d09-b82f-e4d425591941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "class ServerDistiller(keras.Model):\n",
    "    def __init__(self, teacher, student, temp=3.0, alpha=0.1, grad_clip=1.0):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.temp = temp\n",
    "        self.alpha = alpha\n",
    "        self.grad_clip = grad_clip\n",
    "        self.teacher.trainable = False  # Freeze the teacher model\n",
    "\n",
    "        # Metrics\n",
    "        self.total_loss_metric = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.student_loss_metric = keras.metrics.Mean(name=\"student_loss\")\n",
    "        self.distill_loss_metric = keras.metrics.Mean(name=\"distill_loss\")\n",
    "        self.acc_metric = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        \"\"\"Properly compiles the model with loss validation bypass.\"\"\"\n",
    "        kwargs.pop('loss', None)  # Prevent Keras validation issues\n",
    "        super().compile(optimizer=optimizer, loss=self._dummy_loss, **kwargs)\n",
    "\n",
    "        # Define actual loss functions\n",
    "        self.student_loss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)  \n",
    "        self.distill_loss_fn = keras.losses.KLDivergence()\n",
    "\n",
    "    def _dummy_loss(self, y_true, y_pred):\n",
    "        \"\"\"Dummy loss function to bypass Keras validation checks.\"\"\"\n",
    "        return 0.0\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"Custom training step for knowledge distillation.\"\"\"\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            teacher_logits = self.teacher(x, training=False)  # Teacher inference mode\n",
    "            student_probs = self.student(x, training=True)  # Student training mode\n",
    "\n",
    "            # Compute student loss (cross-entropy with true labels)\n",
    "            student_loss = self.student_loss_fn(y, student_probs)\n",
    "\n",
    "            # Compute distillation loss (teacher-student KL divergence)\n",
    "            teacher_probs = tf.nn.softmax(teacher_logits / self.temp, axis=1)\n",
    "            distill_loss = (self.temp ** 2) * self.distill_loss_fn(teacher_probs, student_probs)  # Scale KL divergence\n",
    "\n",
    "            # Total loss: weighted sum of both\n",
    "            total_loss = self.alpha * student_loss + (1 - self.alpha) * distill_loss\n",
    "\n",
    "        # Compute gradients & apply clipping\n",
    "        gradients = tape.gradient(total_loss, self.student.trainable_variables)\n",
    "        gradients = [tf.clip_by_norm(g, self.grad_clip) for g in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.student.trainable_variables))\n",
    "\n",
    "        # Update metrics\n",
    "        self.total_loss_metric.update_state(total_loss)\n",
    "        self.student_loss_metric.update_state(student_loss)\n",
    "        self.distill_loss_metric.update_state(distill_loss)\n",
    "        self.acc_metric.update_state(y, student_probs)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"Returns list of tracked metrics.\"\"\"\n",
    "        return [\n",
    "            self.total_loss_metric,\n",
    "            self.student_loss_metric,\n",
    "            self.distill_loss_metric,\n",
    "            self.acc_metric\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass using student model.\"\"\"\n",
    "        return self.student(inputs, training=training)\n",
    "\n",
    "def plot_kd_metrics(distiller, title_prefix=\"KD Training\"):\n",
    "    epochs = range(1, len(distiller.history_total_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, distiller.history_total_loss, label=\"Total Loss\")\n",
    "    plt.plot(epochs, distiller.history_student_loss, label=\"Student Loss (CE)\")\n",
    "    plt.plot(epochs, distiller.history_distill_loss, label=\"Distillation Loss (KL)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{title_prefix} - Training Losses\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, distiller.history_accuracy, label=\"Train Accuracy\", color=\"green\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{title_prefix} - Train Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba7dacdf-51cb-413e-996b-fb596a6ea81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 441ms/step - accuracy: 0.7141 - distill_loss: 1.0100 - student_loss: 0.6036 - total_loss: 0.9694\n",
      "Epoch 2/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 451ms/step - accuracy: 0.8039 - distill_loss: 0.0090 - student_loss: 0.6545 - total_loss: 0.0735\n",
      "Epoch 3/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 435ms/step - accuracy: 0.8238 - distill_loss: 0.0081 - student_loss: 0.6505 - total_loss: 0.0723\n",
      "Epoch 4/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 425ms/step - accuracy: 0.8142 - distill_loss: 0.0078 - student_loss: 0.6505 - total_loss: 0.0721\n",
      "Epoch 5/5\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 391ms/step - accuracy: 0.8175 - distill_loss: 0.0075 - student_loss: 0.6499 - total_loss: 0.0718\n",
      "Evaluation of distilled student\n",
      "Found 2374 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Major Project\\venv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 222ms/step\n",
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.8256\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Not Drowsy       0.86      0.79      0.82      1223\n",
      "      Drowsy       0.80      0.86      0.83      1151\n",
      "\n",
      "    accuracy                           0.83      2374\n",
      "   macro avg       0.83      0.83      0.83      2374\n",
      "weighted avg       0.83      0.83      0.83      2374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_model=global_model\n",
    "distiller = ServerDistiller(teacher_model, model_student, temp=3.0, alpha=0.1)\n",
    "distiller.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[distiller.acc_metric]\n",
    ")\n",
    "distiller.fit(train_gen, epochs=5, verbose=1)\n",
    "distiller.student.save(r\"D:\\Major Project\\Rasp\\model\\drowsiness_student_post_model_5_epochs_wo_callbacks.keras\")\n",
    "print(\"Evaluation of distilled student\")\n",
    "evaluate_model(distiller.student, test_dir1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc753053-9e72-45b4-a5ee-63983a38a2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
