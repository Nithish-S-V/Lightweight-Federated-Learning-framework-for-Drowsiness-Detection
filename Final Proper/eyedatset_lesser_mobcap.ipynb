{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6a65fb-360a-46a1-ad33-2efa1f6e8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, initializers, backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340cb743-b46b-48f1-a9b5-fcb07f528207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Configuration**\n",
    "INPUT_SHAPE = (224, 224, 3)  # RGB images\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "DATA_DIR = \"D:/Major Project/Final Proper/indian+eye_yawn_dataset\"\n",
    "TRAIN_DIR = \"D:/Major Project/Final Proper/train\"\n",
    "TEST_DIR = \"D:/Major Project/Final Proper/test\"\n",
    "TEST_SIZE = 0.2  # 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1be4853-cc4a-4ae4-9088-c1e3a48e2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# # Define paths\n",
    "# dataset_path = \"D:/Major Project/Final Proper/indian+eye_yawn_dataset\"\n",
    "# train_path = \"D:/Major Project/Final Proper/train\"\n",
    "# test_path = \"D:/Major Project/Final Proper/test\"\n",
    "\n",
    "# # Define split ratio\n",
    "# train_ratio = 0.8  # 80% training, 20% testing\n",
    "\n",
    "# # Create train and test directories\n",
    "# for category in os.listdir(dataset_path):\n",
    "#     category_path = os.path.join(dataset_path, category)\n",
    "    \n",
    "#     if os.path.isdir(category_path):\n",
    "#         images = os.listdir(category_path)\n",
    "#         random.shuffle(images)  # Shuffle before splitting\n",
    "        \n",
    "#         train_size = int(len(images) * train_ratio)\n",
    "        \n",
    "#         train_images = images[:train_size]\n",
    "#         test_images = images[train_size:]\n",
    "        \n",
    "#         # Create category folders in train and test directories\n",
    "#         os.makedirs(os.path.join(train_path, category), exist_ok=True)\n",
    "#         os.makedirs(os.path.join(test_path, category), exist_ok=True)\n",
    "        \n",
    "#         # Move files to train directory\n",
    "#         for img in train_images:\n",
    "#             shutil.copy2(os.path.join(category_path, img), os.path.join(train_path, category, img))\n",
    "        \n",
    "#         # Move files to test directory\n",
    "#         for img in test_images:\n",
    "#             shutil.copy2(os.path.join(category_path, img), os.path.join(test_path, category, img))\n",
    "\n",
    "# print(\"Dataset split completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9269ba0e-3202-4eae-b615-42bd9595bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **Ensure Dataset is RGB**\n",
    "# def convert_images_to_rgb():\n",
    "#     for category in [\"closed_eye\", \"open_eye\"]:\n",
    "#         img_dir = DATA_DIR / category\n",
    "#         for img_path in img_dir.glob(\"*.png\"):  # Adjust for jpg/jpeg if needed\n",
    "#             img = cv2.imread(str(img_path))  # Reads in BGR (already 3 channels)\n",
    "#             if img is None:\n",
    "#                 continue\n",
    "#             cv2.imwrite(str(img_path), img)  # Save back as RGB\n",
    "\n",
    "# convert_images_to_rgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee52f7e5-54d4-4dd6-8cd4-b1f0bd120e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Custom Capsule Layers**\n",
    "class Length(layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "        self.W = self.add_weight(\n",
    "            shape=[1, self.input_num_capsule, self.num_capsule, self.dim_capsule, self.input_dim_capsule],\n",
    "            initializer=initializers.glorot_uniform(),\n",
    "            name='W'\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "        W_tiled = K.tile(self.W, [K.shape(inputs)[0], 1, 1, 1, 1])\n",
    "        inputs_hat = tf.squeeze(tf.matmul(W_tiled, inputs_expand, transpose_b=True), axis=-1)\n",
    "        b = tf.zeros(shape=[K.shape(inputs)[0], self.input_num_capsule, self.num_capsule])\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)\n",
    "            outputs = tf.reduce_sum(inputs_hat * K.expand_dims(c, -1), axis=1)\n",
    "            if i < self.routings - 1:\n",
    "                b += tf.reduce_sum(inputs_hat * K.expand_dims(c, -1), axis=-1)\n",
    "        return outputs\n",
    "\n",
    "# **Build MobileNet + Capsule Model**\n",
    "class MobileNetCapsNet:\n",
    "    def __init__(self, input_shape=(224, 224, 3)):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=self.input_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "        base_model.trainable = False  # Freeze MobileNet\n",
    "\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Reshape((-1, 64))(x)\n",
    "        x = CapsuleLayer(num_capsule=4, dim_capsule=8, routings=1)(x)  # Change num_capsule=4\n",
    "        outputs = Length()(x)\n",
    "\n",
    "        return tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "\n",
    "# **Margin Loss Function**\n",
    "def margin_loss(y_true, y_pred):\n",
    "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=4)  # Change depth=4\n",
    "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9932abd6-a012-4fb8-9cdc-924536004bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **Prepare Training and Testing Data**\n",
    "# def prepare_datasets():\n",
    "#     train_dir = DATA_DIR.parent / \"train\"\n",
    "#     test_dir = DATA_DIR.parent / \"test\"\n",
    "\n",
    "#     # Create directories\n",
    "#     for cls in [\"closed_eye\", \"open_eye\"]:\n",
    "#         (train_dir/cls).mkdir(parents=True, exist_ok=True)\n",
    "#         (test_dir/cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         files = list((DATA_DIR/cls).glob(\"*\"))\n",
    "#         if not files:\n",
    "#             raise ValueError(f\"No files found in {DATA_DIR/cls}\")\n",
    "\n",
    "#         train_files, test_files = train_test_split(files, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "#         for f in train_files:\n",
    "#             shutil.copy(f, train_dir/cls/f.name)\n",
    "#         for f in test_files:\n",
    "#             shutil.copy(f, test_dir/cls/f.name)\n",
    "\n",
    "#     return train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5819caff-b11b-4153-b533-e6099ebc54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Train the Model**\n",
    "def train_model():\n",
    "    train_dir, test_dir = TRAIN_DIR, TEST_DIR\n",
    "\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale = 1.0 / 255,\n",
    "        rotation_range = 15,  # Reduce rotation to keep eye structure  \n",
    "        width_shift_range = 0.15,  # Reduce shift  \n",
    "        height_shift_range = 0.15,  # Reduce shift  \n",
    "        shear_range = 0.2,  \n",
    "        zoom_range = 0.15,  # Reduce zoom  \n",
    "        brightness_range = [0.9, 1.1],  # Keep lighting realistic  \n",
    "        horizontal_flip = True,  # Keep, since it's natural  \n",
    "        vertical_flip = False,  # REMOVE, unnatural for faces  \n",
    "        fill_mode = \"nearest\",\n",
    "        validation_split = 0.2  \n",
    "        )\n",
    "\n",
    "\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=INPUT_SHAPE[:2],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',  # Change from 'binary' to 'categorical'\n",
    "        color_mode='rgb',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    val_gen = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=INPUT_SHAPE[:2],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',  # Change from 'binary' to 'categorical'\n",
    "        color_mode='rgb',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    print(\"Class Indices:\", train_gen.class_indices)\n",
    "\n",
    "    model = MobileNetCapsNet().model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',  # Ensure correct loss function\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # **Test Model**\n",
    "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "    test_gen = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=INPUT_SHAPE[:2],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',  # Change from 'binary' to 'categorical'\n",
    "        color_mode='rgb',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    y_pred = np.argmax(model.predict(test_gen), axis=1)\n",
    "    y_true = test_gen.classes \n",
    "\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"Accuracy: {np.mean(y_true == y_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Eyeclose', 'Happy', 'Neutral', 'Yawn']))\n",
    "\n",
    "    # **Save Model**\n",
    "    model.save(\"drowsiness_lesser_india_plus_eye.keras\")\n",
    "    print(\"\\nModel saved as 'drowsiness_lesser_india_plus_eye.keras' ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b603148-291e-4fce-be02-53bb1cb24c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2381 images belonging to 4 classes.\n",
      "Found 593 images belonging to 4 classes.\n",
      "Class Indices: {'Eyeclose': 0, 'Happy': 1, 'Neutral': 2, 'Yawn': 3}\n",
      "WARNING:tensorflow:From D:\\Major Project\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Major Project\\venv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 238ms/step - accuracy: 0.6913 - loss: 0.8314 - val_accuracy: 0.4368 - val_loss: 1.1547\n",
      "Epoch 2/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 190ms/step - accuracy: 0.7891 - loss: 0.6046 - val_accuracy: 0.5464 - val_loss: 1.0723\n",
      "Epoch 3/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 189ms/step - accuracy: 0.8277 - loss: 0.5436 - val_accuracy: 0.5396 - val_loss: 1.0078\n",
      "Epoch 4/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 189ms/step - accuracy: 0.8508 - loss: 0.5127 - val_accuracy: 0.5025 - val_loss: 1.0948\n",
      "Epoch 5/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 189ms/step - accuracy: 0.8575 - loss: 0.5030 - val_accuracy: 0.5835 - val_loss: 0.9747\n",
      "Epoch 6/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 189ms/step - accuracy: 0.8670 - loss: 0.4670 - val_accuracy: 0.5902 - val_loss: 0.9322\n",
      "Epoch 7/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 191ms/step - accuracy: 0.8816 - loss: 0.4374 - val_accuracy: 0.5885 - val_loss: 1.0017\n",
      "Epoch 8/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 190ms/step - accuracy: 0.8860 - loss: 0.4487 - val_accuracy: 0.5868 - val_loss: 0.9871\n",
      "Epoch 9/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 188ms/step - accuracy: 0.8906 - loss: 0.4432 - val_accuracy: 0.5717 - val_loss: 0.9508\n",
      "Epoch 10/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 189ms/step - accuracy: 0.8974 - loss: 0.4271 - val_accuracy: 0.5734 - val_loss: 0.9718\n",
      "Epoch 11/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 189ms/step - accuracy: 0.8904 - loss: 0.4394 - val_accuracy: 0.5582 - val_loss: 0.9665\n",
      "Epoch 12/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 190ms/step - accuracy: 0.8868 - loss: 0.4341 - val_accuracy: 0.5852 - val_loss: 1.0020\n",
      "Epoch 13/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 188ms/step - accuracy: 0.8960 - loss: 0.4286 - val_accuracy: 0.5784 - val_loss: 0.9514\n",
      "Epoch 14/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 190ms/step - accuracy: 0.8998 - loss: 0.4219 - val_accuracy: 0.6290 - val_loss: 0.9640\n",
      "Epoch 15/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 191ms/step - accuracy: 0.9028 - loss: 0.4289 - val_accuracy: 0.6037 - val_loss: 1.0024\n",
      "Epoch 16/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 188ms/step - accuracy: 0.9027 - loss: 0.4116 - val_accuracy: 0.6037 - val_loss: 0.9582\n",
      "Epoch 17/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 189ms/step - accuracy: 0.8841 - loss: 0.4249 - val_accuracy: 0.6088 - val_loss: 0.9041\n",
      "Epoch 18/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 191ms/step - accuracy: 0.8977 - loss: 0.3979 - val_accuracy: 0.5970 - val_loss: 0.9735\n",
      "Epoch 19/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 187ms/step - accuracy: 0.9038 - loss: 0.4240 - val_accuracy: 0.5953 - val_loss: 1.0103\n",
      "Epoch 20/20\n",
      "\u001b[1m298/298\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 190ms/step - accuracy: 0.9054 - loss: 0.4151 - val_accuracy: 0.5970 - val_loss: 0.9125\n",
      "Found 746 images belonging to 4 classes.\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 135ms/step\n",
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.8056\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Eyeclose       0.97      0.75      0.85       247\n",
      "       Happy       0.72      0.85      0.78       143\n",
      "     Neutral       0.71      0.99      0.83       208\n",
      "        Yawn       0.89      0.61      0.72       148\n",
      "\n",
      "    accuracy                           0.81       746\n",
      "   macro avg       0.82      0.80      0.79       746\n",
      "weighted avg       0.84      0.81      0.80       746\n",
      "\n",
      "\n",
      "Model saved as 'drowsiness_lesser_india_plus_eye.keras' ✅\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6642f8-e5be-4cf6-95ce-688d88a38cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cd236-3510-4c22-a175-0004ea8dee09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
