{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ba9911-9ce9-4f26-9e47-0886999fc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import zlib\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "# from tensorflow.keras.saving import register_keras_serializable\n",
    "from tensorflow.keras import layers, initializers, backend as K\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from cryptography.fernet import Fernet\n",
    "import base64\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea7ea7b-9dd0-475d-b714-8f68aca31df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, initializers, backend as K\n",
    "\n",
    "# **Custom Capsule Layers**\n",
    "class Length(layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "        self.W = self.add_weight(\n",
    "            shape=[1, self.input_num_capsule, self.num_capsule, self.dim_capsule, self.input_dim_capsule],\n",
    "            initializer=initializers.glorot_uniform(),\n",
    "            name='W'\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "        W_tiled = K.tile(self.W, [K.shape(inputs)[0], 1, 1, 1, 1])\n",
    "        inputs_hat = tf.squeeze(tf.matmul(W_tiled, inputs_expand, transpose_b=True), axis=-1)\n",
    "        b = tf.zeros(shape=[K.shape(inputs)[0], self.input_num_capsule, self.num_capsule])\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)\n",
    "            outputs = tf.reduce_sum(inputs_hat * K.expand_dims(c, -1), axis=1)\n",
    "            if i < self.routings - 1:\n",
    "                b += tf.reduce_sum(inputs_hat * K.expand_dims(c, -1), axis=-1)\n",
    "        return outputs\n",
    "\n",
    "# **Build MobileNet + Capsule Model**\n",
    "class MobileNetCapsNet:\n",
    "    def __init__(self, input_shape=(224, 224, 3)):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=self.input_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "        base_model.trainable = False  # Freeze MobileNet\n",
    "\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Reshape((-1, 64))(x)\n",
    "        x = CapsuleLayer(num_capsule=2, dim_capsule=8, routings=1)(x)\n",
    "        outputs = Length()(x)\n",
    "\n",
    "        return tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "# **Margin Loss Function**\n",
    "def margin_loss(y_true, y_pred):\n",
    "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=2)\n",
    "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6daaf27-4081-49ad-98cf-89fff15be1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Major Project\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# custom_objects = {\n",
    "#     'CapsuleLayer': CapsuleLayer,\n",
    "#     'PrimaryCap': PrimaryCap,\n",
    "#     'squash': squash,\n",
    "#     'capsule_length': capsule_length,\n",
    "#     'margin_loss': margin_loss,  # include if you're using a custom margin loss\n",
    "\n",
    "# }\n",
    "model=tf.keras.models.load_model(r\"D:\\Major Project\\Final Proper\\drowsiness_lesser_india_plus_eye.keras\",custom_objects={\"CapsuleLayer\": CapsuleLayer,\"Length\":Length,\"margin_loss\":margin_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce6da336-7b28-4d57-afdb-6df9b5a6fc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4)\n"
     ]
    }
   ],
   "source": [
    "print(model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb0050-a81b-4bd4-b626-0e33f57260d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def extract_frames(video_path, output_folder, frame_interval=10):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        print(f\"Frame {saved_count}: Shape={frame.shape}\")  # Debugging print\n",
    "\n",
    "        if True:  # Force rotation for every frame\n",
    "            print(f\"üîÑ Rotating frame {saved_count}...\")\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "        frame_path = os.path.join(output_folder, f\"frame_{saved_count}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        saved_count += 1\n",
    "\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"‚úÖ Extracted {saved_count} frames with corrected orientation to {output_folder}\")\n",
    "\n",
    "# **Run it on your video**\n",
    "video_path = r\"D:\\Major Project\\Final Proper\\Video_Data\\Fold1_part2\\07\\10.mp4\"  # Update this\n",
    "output_folder = \"american_extracted_frames\"\n",
    "extract_frames(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ce21e-8f6d-4d92-b985-fa7cb30627bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "# **Load Your Pretrained Model**\n",
    "#model = tf.keras.models.load_model(\"your_model_path.h5\")  # Change to your model path\n",
    "\n",
    "# **Define Class Labels**\n",
    "CLASS_LABELS = {\n",
    "    0: \"Eyes Closed\",  # Drowsy\n",
    "    1: \"Happy\",        # Non-Drowsy\n",
    "    2: \"Neutral\",      # Non-Drowsy\n",
    "    3: \"Yawning\"       # Drowsy\n",
    "}\n",
    "\n",
    "# **Setup MediaPipe Face Mesh**\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# **Dataset Paths**\n",
    "input_folder = \"american_extracted_frames\"\n",
    "drowsy_folder = \"drowsy\"\n",
    "non_drowsy_folder = \"non_drowsy\"\n",
    "uncertain_folder = \"uncertain\"\n",
    "\n",
    "os.makedirs(drowsy_folder, exist_ok=True)\n",
    "os.makedirs(non_drowsy_folder, exist_ok=True)\n",
    "os.makedirs(uncertain_folder, exist_ok=True)\n",
    "\n",
    "# **Preprocessing Function for Model**\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = cv2.imread(image_path)\n",
    "    #img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img / 255.0  # Normalize\n",
    "    return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# **Function to Check Head Orientation**\n",
    "def get_head_tilt(face_landmarks, image_width):\n",
    "    left_cheek = face_landmarks.landmark[234].x * image_width\n",
    "    right_cheek = face_landmarks.landmark[454].x * image_width\n",
    "    face_center = (left_cheek + right_cheek) / 2\n",
    "\n",
    "    if face_center < image_width * 0.35:\n",
    "        return \"Right\"\n",
    "    elif face_center > image_width * 0.65:\n",
    "        return \"Left\"\n",
    "    else:\n",
    "        return \"Center\"\n",
    "\n",
    "# **Function to Process Image and Classify Drowsiness**\n",
    "def classify_drowsiness(img, img_path):\n",
    "    h, w, _ = img.shape\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_img)\n",
    "\n",
    "    avg_eye_opening = None\n",
    "    mouth_opening = None\n",
    "    head_orientation = \"Center\"\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            head_orientation = get_head_tilt(face_landmarks, w)\n",
    "\n",
    "            # **Extract Eye Landmarks**\n",
    "            left_eye_top = face_landmarks.landmark[159]\n",
    "            left_eye_bottom = face_landmarks.landmark[145]\n",
    "            right_eye_top = face_landmarks.landmark[386]\n",
    "            right_eye_bottom = face_landmarks.landmark[374]\n",
    "\n",
    "            if head_orientation == \"Right\":\n",
    "                avg_eye_opening = (left_eye_bottom.y - left_eye_top.y) * h\n",
    "            elif head_orientation == \"Left\":\n",
    "                avg_eye_opening = (right_eye_bottom.y - right_eye_top.y) * h\n",
    "            else:\n",
    "                avg_eye_opening = ((left_eye_bottom.y - left_eye_top.y) + (right_eye_bottom.y - right_eye_top.y)) * h / 2\n",
    "\n",
    "            # **Mouth Landmarks**\n",
    "            upper_lip_top = face_landmarks.landmark[13]\n",
    "            lower_lip_bottom = face_landmarks.landmark[14]\n",
    "            mouth_opening = (lower_lip_bottom.y - upper_lip_top.y) * h\n",
    "\n",
    "    # **Preprocess Image for Model**\n",
    "    img_array = preprocess_image(img_path)\n",
    "\n",
    "    # **Model Prediction**\n",
    "    prediction = model.predict(img_array)\n",
    "    class_id = np.argmax(prediction)\n",
    "    confidence = round(max(prediction[0]) * 100, 2)\n",
    "\n",
    "    # **Fusion of MediaPipe & Model Results**\n",
    "    eye_threshold = 5\n",
    "    mouth_threshold = 15\n",
    "\n",
    "    if avg_eye_opening is not None and avg_eye_opening < eye_threshold:\n",
    "        detected_drowsy = True\n",
    "    elif mouth_opening is not None and mouth_opening > mouth_threshold:\n",
    "        detected_drowsy = True\n",
    "    else:\n",
    "        detected_drowsy = class_id in [0, 3]\n",
    "\n",
    "    # **Final Decision**\n",
    "    if detected_drowsy:\n",
    "        target_folder = drowsy_folder\n",
    "        status = \"Drowsy\"\n",
    "        color = (0, 0, 255)\n",
    "    else:\n",
    "        target_folder = non_drowsy_folder\n",
    "        status = \"Non-Drowsy\"\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "    # **Store Image in Folder Based on Confidence**\n",
    "    if confidence >= 75:\n",
    "        print(f\"‚úÖ Image: {img_path} ‚Üí {CLASS_LABELS[class_id]} ({status}) (Confidence: {confidence}%)\")\n",
    "        shutil.copy2(img_path, os.path.join(target_folder, os.path.basename(img_path)))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Uncertain: {img_path} (Confidence: {confidence}%) - Below threshold\")\n",
    "        shutil.copy2(img_path, os.path.join(uncertain_folder, os.path.basename(img_path)))\n",
    "\n",
    "# **Process All Images in Folder**\n",
    "for filename in os.listdir(input_folder):\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        classify_drowsiness(img, img_path)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping {filename} (Could not read image)\")\n",
    "\n",
    "print(\"\\n‚úÖ Classification completed! High-confidence images sorted, uncertain ones stored separately.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5bc209-a21e-465e-9839-f85f160a4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_indices = model.class_indices\n",
    "# print(f\"Class Indices from Model: {class_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38619e35-9734-4fe7-b52d-065ca2f7a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# # Image preprocessing function\n",
    "# def preprocess_image(image_path, target_size=(224, 224)):\n",
    "#     img = cv2.imread(image_path)  # Read the image\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "#     img = cv2.resize(img, target_size)  # Resize to model input size\n",
    "#     img = img / 255.0  # Normalize pixel values (0 to 1)\n",
    "#     return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# # Function to classify a single image\n",
    "# def classify_image(image_path, confidence_threshold=75):\n",
    "#     img_array = preprocess_image(image_path)\n",
    "\n",
    "#     # Predict using the model\n",
    "#     prediction = model.predict(img_array)\n",
    "#     class_id = np.argmax(prediction)  # Get the predicted class index\n",
    "#     confidence = round(max(prediction[0]) * 100, 2)  # Get highest probability\n",
    "\n",
    "#     # **Corrected Mapping: Closed Eyes (0) = Drowsy, Open Eyes (1) = Non-Drowsy**\n",
    "#     label = \"Drowsy üò¥\" if class_id == 0 else \"Non-Drowsy üòÉ\"\n",
    "\n",
    "#     # Print result\n",
    "#     print(f\"üñºÔ∏è Image: {image_path}\")\n",
    "#     print(f\"üîç Prediction: {label} (Confidence: {confidence}%)\")\n",
    "\n",
    "#     # Return results\n",
    "#     return {\"image\": image_path, \"label\": label, \"confidence\": confidence}\n",
    "\n",
    "# # Test the function with an image\n",
    "# image_path = r\"D:\\Major Project\\Final Proper\\Dataset\\American\\Drowsy\\img_e_15341.jpg\"# Change this to your image file path\n",
    "# result = classify_image(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf88b47-1801-4f62-993a-7103ab7e0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # **Define Class Labels (Matching Your Dataset)**\n",
    "# CLASS_LABELS = {\n",
    "#     0: \"Eyes Closed\",   # Drowsy (Mapped from \"Eyeclose\")\n",
    "#     1: \"Happy\",         # Non-Drowsy\n",
    "#     2: \"Neutral\",       # Non-Drowsy\n",
    "#     3: \"Yawning\"        # Drowsy (Mapped from \"Yawn\")\n",
    "# }\n",
    "\n",
    "# # **Preprocess Image Before Prediction**\n",
    "# def preprocess_image(image_path, target_size=(224, 224)):\n",
    "#     img = cv2.imread(image_path)  # Read the image\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "#     img = cv2.resize(img, target_size)  # Resize to match model input\n",
    "#     img = img / 255.0  # Normalize pixel values (0 to 1)\n",
    "#     return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# # **Classify Image Function**\n",
    "# def classify_image(image_path):\n",
    "#     img_array = preprocess_image(image_path)\n",
    "\n",
    "#     # Predict using the model\n",
    "#     prediction = model.predict(img_array)\n",
    "#     class_id = np.argmax(prediction)  # Get the predicted class index\n",
    "#     confidence = round(max(prediction[0])*10 , 2)  # Get highest probability\n",
    "\n",
    "#     # Determine drowsiness status\n",
    "#     if class_id in [0, 3]:  # Eyes Closed or Yawning\n",
    "#         status = \"Drowsy üò¥\"\n",
    "#     else:  # Happy or Neutral\n",
    "#         status = \"Non-Drowsy üòÉ\"\n",
    "\n",
    "#     # Print result\n",
    "#     print(f\"üñºÔ∏è Image: {image_path}\")\n",
    "#     print(f\"üîç Prediction: {CLASS_LABELS[class_id]} ({status}) (Confidence: {confidence}%)\")\n",
    "\n",
    "#     # Return results\n",
    "#     return {\"image\": image_path, \"label\": CLASS_LABELS[class_id], \"status\": status, \"confidence\": confidence}\n",
    "\n",
    "# # **Example Usage**\n",
    "# image_path = r\"D:\\Major Project\\Dataset_Classified\\non_drowsy\\Drowsy (4458).jpg\" # Change to your test image path\n",
    "# result = classify_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26672428-3efb-495d-881c-f02bb1be3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT USING MEDIAPIPE\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import shutil\n",
    "\n",
    "\n",
    "# # **Define Dataset Paths**\n",
    "# input_folder = \"D:/Major Project/Dataset\"  # Root dataset folder\n",
    "# drowsy_folder = \"D:/Major Project/Dataset_Classified/drowsy\"\n",
    "# non_drowsy_folder = \"D:/Major Project/Dataset_Classified/non_drowsy\"\n",
    "# uncertain_folder = \"D:/Major Project/Dataset_Classified/uncertain\"  # Low-confidence images\n",
    "\n",
    "# # **Create Output Folders if They Don't Exist**\n",
    "# os.makedirs(drowsy_folder, exist_ok=True)\n",
    "# os.makedirs(non_drowsy_folder, exist_ok=True)\n",
    "# os.makedirs(uncertain_folder, exist_ok=True)\n",
    "\n",
    "# # **Class Labels Mapping (Based on Your Dataset)**\n",
    "# CLASS_LABELS = {\n",
    "#     0: \"Eyes Closed\",   # Drowsy\n",
    "#     1: \"Happy\",         # Non-Drowsy\n",
    "#     2: \"Neutral\",       # Non-Drowsy\n",
    "#     3: \"Yawning\"        # Drowsy\n",
    "# }\n",
    "\n",
    "# # **Preprocessing Function**\n",
    "# def preprocess_image(image_path, target_size=(224, 224)):\n",
    "#     img = cv2.imread(image_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     img = cv2.resize(img, target_size)\n",
    "#     img = img / 255.0  # Normalize\n",
    "#     return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# # **Classify and Sort Images**\n",
    "# CONFIDENCE_THRESHOLD = 75  # Confidence cutoff for certainty\n",
    "\n",
    "# for subdir, _, files in os.walk(input_folder):\n",
    "#     for filename in files:\n",
    "#         img_path = os.path.join(subdir, filename)\n",
    "\n",
    "#         # **Skip Non-Image Files**\n",
    "#         if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "#             continue\n",
    "\n",
    "#         img_array = preprocess_image(img_path)\n",
    "\n",
    "#         # **Predict Using Model**\n",
    "#         prediction = model.predict(img_array)\n",
    "#         class_id = np.argmax(prediction)  # Get class index\n",
    "#         confidence = round(max(prediction[0]) * 100, 2)  # Get highest probability\n",
    "\n",
    "#         # **Determine Drowsiness Status**\n",
    "#         if class_id in [0, 3]:  # Eyes Closed or Yawning\n",
    "#             status = \"Drowsy\"\n",
    "#             target_folder = drowsy_folder\n",
    "#         else:  # Happy or Neutral\n",
    "#             status = \"Non-Drowsy\"\n",
    "#             target_folder = non_drowsy_folder\n",
    "\n",
    "#         # **Sort Based on Confidence Level**\n",
    "#         if confidence >= CONFIDENCE_THRESHOLD:\n",
    "#             print(f\"‚úÖ {filename} ‚Üí {CLASS_LABELS[class_id]} ({status}) (Confidence: {confidence}%)\")\n",
    "#             shutil.copy2(img_path, os.path.join(target_folder, filename))\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è Uncertain: {filename} (Confidence: {confidence}%) - Below threshold\")\n",
    "#             shutil.copy2(img_path, os.path.join(uncertain_folder, filename))\n",
    "\n",
    "# print(\"\\n‚úÖ Classification completed! High-confidence images sorted, uncertain ones stored separately.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67583fab-6b42-4b20-bf4f-9e461742bd3c",
   "metadata": {},
   "source": [
    "##USING MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d90c4-efb0-4d22-8045-bdbb8cd4773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import mediapipe as mp\n",
    "\n",
    "# **Load MediaPipe Face Mesh**\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# **Load Pretrained Model**\n",
    "model = model# Ensure the path is correct\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# **Get Input Folder from User**\n",
    "input_folder = r\"D:\\Major Project\\Dataset\\notdrowsy\"  # Update the input directory\n",
    "\n",
    "# **Check if the directory exists**\n",
    "if not os.path.exists(input_folder):\n",
    "    print(f\"‚ùå Error: The directory '{input_folder}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# **Define Output Folders**\n",
    "output_base = os.path.join(input_folder, \"Dataset_Classified\")\n",
    "drowsy_folder = os.path.join(output_base, \"drowsy\")\n",
    "non_drowsy_folder = os.path.join(output_base, \"non_drowsy\")\n",
    "uncertain_folder = os.path.join(output_base, \"uncertain\")\n",
    "\n",
    "# **Create Output Folders if They Don't Exist**\n",
    "os.makedirs(drowsy_folder, exist_ok=True)\n",
    "os.makedirs(non_drowsy_folder, exist_ok=True)\n",
    "os.makedirs(uncertain_folder, exist_ok=True)\n",
    "\n",
    "# **Class Labels Mapping**\n",
    "CLASS_LABELS = {\n",
    "    0: \"Eyes Closed\",   # Drowsy\n",
    "    1: \"Happy\",         # Non-Drowsy\n",
    "    2: \"Neutral\",       # Non-Drowsy\n",
    "    3: \"Yawning\"        # Drowsy\n",
    "}\n",
    "\n",
    "# **Preprocessing Function**\n",
    "def preprocess_image(img, target_size=(224, 224)):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img / 255.0  # Normalize\n",
    "    return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# **Confidence Threshold**\n",
    "CONFIDENCE_THRESHOLD = 75  \n",
    "\n",
    "# **Process Images in the Input Folder**\n",
    "for subdir, _, files in os.walk(input_folder):\n",
    "    for filename in files:\n",
    "        img_path = os.path.join(subdir, filename)\n",
    "\n",
    "        # **Skip Non-Image Files**\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Skipping {filename} (Could not read image)\")\n",
    "            continue\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        # **Detect face mesh using MediaPipe**\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_img)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # **Extract Eye Landmarks**\n",
    "                left_eye_top = face_landmarks.landmark[159]\n",
    "                left_eye_bottom = face_landmarks.landmark[145]\n",
    "                right_eye_top = face_landmarks.landmark[386]\n",
    "                right_eye_bottom = face_landmarks.landmark[374]\n",
    "\n",
    "                left_eye_opening = int((left_eye_bottom.y - left_eye_top.y) * h)\n",
    "                right_eye_opening = int((right_eye_bottom.y - right_eye_top.y) * h)\n",
    "\n",
    "                # **Eye openness threshold**\n",
    "                eye_threshold = 5  # Adjust based on testing\n",
    "                avg_eye_opening = (left_eye_opening + right_eye_opening) / 2\n",
    "\n",
    "                # **Extract Mouth Landmarks**\n",
    "                upper_lip_top = face_landmarks.landmark[13]\n",
    "                lower_lip_bottom = face_landmarks.landmark[14]\n",
    "\n",
    "                mouth_opening = int((lower_lip_bottom.y - upper_lip_top.y) * h)\n",
    "\n",
    "                # **Preprocess Image & Predict Using Model**\n",
    "                img_array = preprocess_image(img)\n",
    "                prediction = model.predict(img_array)  # Model prediction\n",
    "                class_id = np.argmax(prediction)  # Get class index\n",
    "                confidence = round(max(prediction[0]) * 100, 2)  # Get highest probability\n",
    "\n",
    "                # **Drowsiness Classification Logic**\n",
    "                if avg_eye_opening < eye_threshold or class_id == 0:\n",
    "                    status = \"Drowsy\"\n",
    "                    target_folder = drowsy_folder\n",
    "                elif mouth_opening > 15 and class_id == 3:\n",
    "                    status = \"Drowsy\"\n",
    "                    target_folder = drowsy_folder\n",
    "                else:\n",
    "                    status = \"Non-Drowsy\"\n",
    "                    target_folder = non_drowsy_folder\n",
    "\n",
    "                # **Sort Based on Confidence Level**\n",
    "                if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                    print(f\"‚úÖ {filename} ‚Üí {CLASS_LABELS[class_id]} ({status}) (Confidence: {confidence}%)\")\n",
    "                    shutil.copy2(img_path, os.path.join(target_folder, filename))\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Uncertain: {filename} (Confidence: {confidence}%) - Below threshold\")\n",
    "                    shutil.copy2(img_path, os.path.join(uncertain_folder, filename))\n",
    "        else:\n",
    "            print(f\"üö´ No face detected in {filename}. Marking as uncertain.\")\n",
    "            shutil.copy2(img_path, os.path.join(uncertain_folder, filename))\n",
    "\n",
    "print(\"\\n‚úÖ Classification completed! High-confidence images sorted, uncertain ones stored separately.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad70d46-0e99-460b-b59c-8705230634ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import shutil\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # **Load MediaPipe Face Mesh**\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "# face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# # **Load Pretrained Model**\n",
    "# #model = tf.keras.models.load_model(\"D:/Major Project/your_model.h5\")  # Update with actual model path\n",
    "\n",
    "# # **Define Dataset Paths**\n",
    "# input_folder = \"D:/Major Project/Dataset\"  # Root dataset folder\n",
    "# drowsy_folder = \"D:/Major Project/Dataset_Classified/drowsy\"\n",
    "# non_drowsy_folder = \"D:/Major Project/Dataset_Classified/non_drowsy\"\n",
    "# uncertain_folder = \"D:/Major Project/Dataset_Classified/uncertain\"  # Low-confidence images\n",
    "\n",
    "# # **Create Output Folders if They Don't Exist**\n",
    "# os.makedirs(drowsy_folder, exist_ok=True)\n",
    "# os.makedirs(non_drowsy_folder, exist_ok=True)\n",
    "# os.makedirs(uncertain_folder, exist_ok=True)\n",
    "\n",
    "# # **Class Labels Mapping (Based on Your Dataset)**\n",
    "# CLASS_LABELS = {\n",
    "#     0: \"Eyes Closed\",   # Drowsy\n",
    "#     1: \"Happy\",         # Non-Drowsy\n",
    "#     2: \"Neutral\",       # Non-Drowsy\n",
    "#     3: \"Yawning\"        # Drowsy\n",
    "# }\n",
    "\n",
    "# # **Preprocessing Function**\n",
    "# def preprocess_image(img, target_size=(224, 224)):\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     img = cv2.resize(img, target_size)\n",
    "#     img = img / 255.0  # Normalize\n",
    "#     return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# # **Classify and Sort Images**\n",
    "# CONFIDENCE_THRESHOLD = 75  # Confidence cutoff for certainty\n",
    "\n",
    "# for subdir, _, files in os.walk(input_folder):\n",
    "#     for filename in files:\n",
    "#         img_path = os.path.join(subdir, filename)\n",
    "\n",
    "#         # **Skip Non-Image Files**\n",
    "#         if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "#             continue\n",
    "\n",
    "#         img = cv2.imread(img_path)\n",
    "#         if img is None:\n",
    "#             print(f\"‚ö†Ô∏è Skipping {filename} (Could not read image)\")\n",
    "#             continue\n",
    "\n",
    "#         h, w, _ = img.shape\n",
    "\n",
    "#         # **Detect face mesh using MediaPipe**\n",
    "#         rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         results = face_mesh.process(rgb_img)\n",
    "\n",
    "#         if results.multi_face_landmarks:\n",
    "#             for face_landmarks in results.multi_face_landmarks:\n",
    "#                 # **Extract Eye Landmarks**\n",
    "#                 left_eye_top = face_landmarks.landmark[159]\n",
    "#                 left_eye_bottom = face_landmarks.landmark[145]\n",
    "#                 right_eye_top = face_landmarks.landmark[386]\n",
    "#                 right_eye_bottom = face_landmarks.landmark[374]\n",
    "\n",
    "#                 left_eye_opening = int((left_eye_bottom.y - left_eye_top.y) * h)\n",
    "#                 right_eye_opening = int((right_eye_bottom.y - right_eye_top.y) * h)\n",
    "\n",
    "#                 # **Eye openness threshold**\n",
    "#                 eye_threshold = 5  # Adjust based on testing\n",
    "#                 avg_eye_opening = (left_eye_opening + right_eye_opening) / 2\n",
    "\n",
    "#                 # **Extract Mouth Landmarks**\n",
    "#                 upper_lip_top = face_landmarks.landmark[13]\n",
    "#                 lower_lip_bottom = face_landmarks.landmark[14]\n",
    "\n",
    "#                 mouth_opening = int((lower_lip_bottom.y - upper_lip_top.y) * h)\n",
    "\n",
    "#                 # **Preprocess and Predict Using Model**\n",
    "#                 img_array = preprocess_image(img)\n",
    "#                 prediction = model.predict(img_array)\n",
    "#                 class_id = np.argmax(prediction)  # Get class index\n",
    "#                 confidence = round(max(prediction[0]) * 100, 2)  # Get highest probability\n",
    "\n",
    "#                 # **Drowsiness Classification Logic**\n",
    "#                 if avg_eye_opening < eye_threshold or class_id == 0:\n",
    "#                     status = \"Drowsy\"\n",
    "#                     target_folder = drowsy_folder\n",
    "#                 elif mouth_opening > 15 and class_id == 3:\n",
    "#                     status = \"Drowsy\"\n",
    "#                     target_folder = drowsy_folder\n",
    "#                 else:\n",
    "#                     status = \"Non-Drowsy\"\n",
    "#                     target_folder = non_drowsy_folder\n",
    "\n",
    "#                 # **Sort Based on Confidence Level**\n",
    "#                 if confidence >= CONFIDENCE_THRESHOLD:\n",
    "#                     print(f\"‚úÖ {filename} ‚Üí {CLASS_LABELS[class_id]} ({status}) (Confidence: {confidence}%)\")\n",
    "#                     shutil.copy2(img_path, os.path.join(target_folder, filename))\n",
    "#                 else:\n",
    "#                     print(f\"‚ö†Ô∏è Uncertain: {filename} (Confidence: {confidence}%) - Below threshold\")\n",
    "#                     shutil.copy2(img_path, os.path.join(uncertain_folder, filename))\n",
    "#         else:\n",
    "#             print(f\"üö´ No face detected in {filename}. Marking as uncertain.\")\n",
    "#             shutil.copy2(img_path, os.path.join(uncertain_folder, filename))\n",
    "\n",
    "# print(\"\\n‚úÖ Classification completed! High-confidence images sorted, uncertain ones stored separately.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967aa9e-3fa6-4a56-8504-2a6563008797",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf6abf-b1eb-4dd5-b4fd-a1a0b845b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########---------------------------WORKIG ONE------------------------------------#######\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import mediapipe as mp\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # **Load MediaPipe Face Mesh**\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "# face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# # **Define class labels**\n",
    "# CLASS_LABELS = {\n",
    "#     0: \"Eyes Closed\",  # Drowsy\n",
    "#     1: \"Happy\",        # Non-Drowsy\n",
    "#     2: \"Neutral\",      # Non-Drowsy\n",
    "#     3: \"Yawning\"       # Drowsy\n",
    "# }\n",
    "\n",
    "# # **Start the webcam**\n",
    "# cap = cv2.VideoCapture(0)  # 0 for default camera\n",
    "\n",
    "# # **Function to preprocess the frame for model input**\n",
    "# def preprocess_frame(frame, target_size=(224, 224)):\n",
    "#     img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     img = cv2.resize(img, target_size)\n",
    "#     img = img / 255.0  # Normalize pixel values (0 to 1)\n",
    "#     return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         print(\"‚ùå Error: Unable to capture video\")\n",
    "#         break\n",
    "\n",
    "#     # **Convert frame to RGB for MediaPipe**\n",
    "#     rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = face_mesh.process(rgb_frame)\n",
    "\n",
    "#     if results.multi_face_landmarks:\n",
    "#         for face_landmarks in results.multi_face_landmarks:\n",
    "#             h, w, _ = frame.shape\n",
    "\n",
    "#             # **Get eye landmarks (based on MediaPipe Face Mesh indices)**\n",
    "#             left_eye_top = face_landmarks.landmark[159]  # Upper eyelid\n",
    "#             left_eye_bottom = face_landmarks.landmark[145]  # Lower eyelid\n",
    "\n",
    "#             right_eye_top = face_landmarks.landmark[386]  # Upper eyelid\n",
    "#             right_eye_bottom = face_landmarks.landmark[374]  # Lower eyelid\n",
    "\n",
    "#             # Convert normalized coordinates to pixels\n",
    "#             left_eye_opening = int((left_eye_bottom.y - left_eye_top.y) * h)\n",
    "#             right_eye_opening = int((right_eye_bottom.y - right_eye_top.y) * h)\n",
    "\n",
    "#             # **Calculate average eye openness**\n",
    "#             avg_eye_opening = (left_eye_opening + right_eye_opening) / 2\n",
    "\n",
    "#             # **Get mouth landmarks**\n",
    "#             upper_lip_top = face_landmarks.landmark[13]  \n",
    "#             lower_lip_bottom = face_landmarks.landmark[14]  \n",
    "\n",
    "#             upper_y = int(upper_lip_top.y * h)\n",
    "#             lower_y = int(lower_lip_bottom.y * h)\n",
    "\n",
    "#             # **Calculate mouth opening**\n",
    "#             mouth_opening = lower_y - upper_y\n",
    "\n",
    "#             # **Preprocess and predict using the model**\n",
    "#             img_array = preprocess_frame(frame)\n",
    "#             prediction = model.predict(img_array)\n",
    "#             class_id = np.argmax(prediction)  # Get class index\n",
    "#             confidence = round(max(prediction[0]) * 100, 2)  # Get highest probability\n",
    "\n",
    "#             # **Drowsiness Classification Logic**\n",
    "#             if avg_eye_opening < 5 or class_id == 0:  # If eyes are too closed OR model says eyes closed\n",
    "#                 status = \"Drowsy üò¥ (Eyes Closed)\"\n",
    "#                 color = (0, 0, 255)  # Red for drowsy\n",
    "#             elif mouth_opening > 15 and class_id == 3:  # If mouth is open + model says yawning\n",
    "#                 status = \"Drowsy üò¥ (Yawning)\"\n",
    "#                 color = (0, 0, 255)\n",
    "#             else:\n",
    "#                 status = \"Non-Drowsy üòä\"\n",
    "#                 color = (0, 255, 0)  # Green for non-drowsy\n",
    "\n",
    "#             # **Display Results on Screen**\n",
    "#             text = f\"{CLASS_LABELS[class_id]} ({status}) - {confidence}%\"\n",
    "#             cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "#     # **Show Webcam Output**\n",
    "#     cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "\n",
    "#     # **Press 'q' to exit**\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # **Release resources**\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a37fe-5c99-4e2d-bc46-326f4637afd2",
   "metadata": {},
   "source": [
    "# ########---------WORKING Perfectly----------#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f926e-fdac-48ed-a610-893e2949122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "# **Load MediaPipe Face Mesh**\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# **Define class labels**\n",
    "CLASS_LABELS = {\n",
    "    0: \"Eyes Closed\",  # Drowsy\n",
    "    1: \"Happy\",        # Non-Drowsy\n",
    "    2: \"Neutral\",      # Non-Drowsy\n",
    "    3: \"Yawning\"       # Drowsy\n",
    "}\n",
    "\n",
    "# **Start the webcam**\n",
    "cap = cv2.VideoCapture(0)  # 0 for default camera\n",
    "\n",
    "# **Function to preprocess the frame for model input**\n",
    "def preprocess_frame(frame, target_size=(224, 224)):\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img / 255.0  # Normalize pixel values (0 to 1)\n",
    "    return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"‚ùå Error: Unable to capture video\")\n",
    "        break\n",
    "\n",
    "    # **Convert frame to RGB for MediaPipe**\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            # **Get eye landmarks**\n",
    "            left_eye_top = face_landmarks.landmark[159]  \n",
    "            left_eye_bottom = face_landmarks.landmark[145]  \n",
    "            right_eye_top = face_landmarks.landmark[386]  \n",
    "            right_eye_bottom = face_landmarks.landmark[374]  \n",
    "\n",
    "            # Convert normalized coordinates to pixels\n",
    "            left_eye_opening = int((left_eye_bottom.y - left_eye_top.y) * h)\n",
    "            right_eye_opening = int((right_eye_bottom.y - right_eye_top.y) * h)\n",
    "\n",
    "            # **Set optimized eye openness threshold**\n",
    "            eye_threshold = 5  # Adjust this based on testing\n",
    "\n",
    "            # **Calculate average eye openness**\n",
    "            avg_eye_opening = (left_eye_opening + right_eye_opening) / 2\n",
    "\n",
    "            # **Get mouth landmarks**\n",
    "            upper_lip_top = face_landmarks.landmark[13]  \n",
    "            lower_lip_bottom = face_landmarks.landmark[14]  \n",
    "\n",
    "            # Convert to pixels\n",
    "            upper_y = int(upper_lip_top.y * h)\n",
    "            lower_y = int(lower_lip_bottom.y * h)\n",
    "\n",
    "            # **Calculate mouth opening**\n",
    "            mouth_opening = lower_y - upper_y\n",
    "\n",
    "            # **Preprocess and predict using the model**\n",
    "            img_array = preprocess_frame(frame)\n",
    "            prediction = model.predict(img_array)\n",
    "            class_id = np.argmax(prediction)  # Get class index\n",
    "            confidence = round(max(prediction[0]) * 100, 2)  # Get highest probability\n",
    "\n",
    "            # **Drowsiness Classification Logic**\n",
    "            if avg_eye_opening < eye_threshold or class_id == 0:\n",
    "                status = \"Drowsy üò¥ (Eyes Closed)\"\n",
    "                color = (0, 0, 255)  # Red for drowsy\n",
    "            elif mouth_opening > 15 and class_id == 3:\n",
    "                status = \"Drowsy üò¥ (Yawning)\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"Non-Drowsy üòä\"\n",
    "                color = (0, 255, 0)  # Green for non-drowsy\n",
    "\n",
    "            # **Display Results on Screen**\n",
    "            text = f\"{CLASS_LABELS[class_id]} ({status}) - {confidence}%\"\n",
    "            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    # **Show Webcam Output**\n",
    "    cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "\n",
    "    # **Press 'q' to exit**\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# **Release resources**\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c98ea-9c85-4401-8b39-79474637e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # **Define class labels**\n",
    "# CLASS_LABELS = {\n",
    "#     0: \"Eyes Closed\",  # Drowsy\n",
    "#     1: \"Happy\",        # Non-Drowsy\n",
    "#     2: \"Neutral\",      # Non-Drowsy\n",
    "#     3: \"Yawning\"       # Drowsy\n",
    "# }\n",
    "\n",
    "# # **Load Haarcascade Classifiers**\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "# mouth_cascade = cv2.CascadeClassifier(r\"D:\\Major Project\\Final Proper\\haarcascade_mcs_mouth.xml\")  # Mouth detection\n",
    "\n",
    "# # **Check if mouth cascade is loaded**\n",
    "# if mouth_cascade.empty():\n",
    "#     print(\"‚ùå Error: Mouth cascade file not loaded! Check the file path.\")\n",
    "#     exit()\n",
    "# else:\n",
    "#     print(\"‚úÖ Mouth cascade loaded successfully!\")\n",
    "\n",
    "# # **Start the webcam**\n",
    "# cap = cv2.VideoCapture(0)  # 0 for default camera\n",
    "\n",
    "# # **Function to preprocess the frame for model input**\n",
    "# def preprocess_frame(frame, target_size=(224, 224)):\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     gray = cv2.equalizeHist(gray)  # Improve contrast\n",
    "#     img = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)  # Convert back to RGB\n",
    "#     img = cv2.resize(img, target_size)\n",
    "#     img = img / 255.0  # Normalize pixel values (0 to 1)\n",
    "#     return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         print(\"‚ùå Error: Unable to capture video\")\n",
    "#         break\n",
    "\n",
    "#     # **Face detection**\n",
    "#     faces = face_cascade.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "#     for (x, y, w, h) in faces:\n",
    "#         roi_face = frame[y:y+h, x:x+w]  # Extract face region\n",
    "        \n",
    "#         # **Eye detection**\n",
    "#         eyes = eye_cascade.detectMultiScale(roi_face)\n",
    "        \n",
    "#         # **Mouth detection (focus on lower 40% of the face)**\n",
    "#         mouth_y_start = int(0.6 * h)  # Start from 60% down\n",
    "#         mouth_region = roi_face[int(h * 0.4):h, :]\n",
    "        \n",
    "#         mouths = mouth_cascade.detectMultiScale(mouth_region, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "#         # **Preprocess and predict using the model**\n",
    "#         img_array = preprocess_frame(roi_face)\n",
    "#         prediction = model.predict(img_array)\n",
    "#         class_id = np.argmax(prediction)  # Get class index\n",
    "#         confidence = round(max(prediction[0]) * 100, 2)  # Get highest probability\n",
    "\n",
    "#         # **Debugging: Print raw predictions**\n",
    "#         print(f\"üîç Raw Prediction: {prediction}\")\n",
    "\n",
    "#         # **Drowsiness Classification Logic**\n",
    "#         if class_id in [0, 3]:  # Eyes Closed OR Yawning\n",
    "#             status = \"Drowsy üò¥\"\n",
    "#             color = (0, 0, 255)  # Red for drowsy\n",
    "#         else:\n",
    "#             status = \"Non-Drowsy üòä\"\n",
    "#             color = (0, 255, 0)  # Green for non-drowsy\n",
    "\n",
    "#         # **Additional Check: If no eyes detected, assume Drowsy**\n",
    "#         if len(eyes) == 0:\n",
    "#             status = \"Drowsy üò¥ (Eyes Not Detected)\"\n",
    "#             color = (0, 0, 255)\n",
    "\n",
    "#         # **Yawn Detection: If mouth is detected and model predicts 'Yawning'**\n",
    "#         if len(mouths) > 0 and class_id == 3:\n",
    "#             status = \"Drowsy üò¥ (Yawning Detected)\"\n",
    "#             color = (0, 0, 255)\n",
    "\n",
    "#         # **Display Results on Screen**\n",
    "#         text = f\"{CLASS_LABELS[class_id]} ({status}) - {confidence}%\"\n",
    "#         cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "#     # **Show Webcam Output**\n",
    "#     cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "\n",
    "#     # **Press 'q' to exit**\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # **Release resources**\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc99b25-cf0d-4f6f-ad00-5ca63ddf445b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
